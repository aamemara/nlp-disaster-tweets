{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Git-Hub URL**"
      ],
      "metadata": {
        "id": "WaWomwMEuzFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/aamemara/nlp-disaster-tweets.git"
      ],
      "metadata": {
        "id": "fp1jZCJXu250"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Imports**"
      ],
      "metadata": {
        "id": "opR1kPVTu3Rh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7Va2u8DNSpTZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import SpatialDropout1D, Embedding, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EDA**"
      ],
      "metadata": {
        "id": "sUeEEOu_vQHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive to load files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnahaxQEboyc",
        "outputId": "d53bdcc2-f8aa-4a0d-9e25-f4dcced20de3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = \"drive/MyDrive/Colab Notebooks/University of Colorado Boulder:  Introduction to Deep Learning/\"\n",
        "print(os.listdir(drive_path+\"nlp-getting-started\"))\n",
        "df_train_val = pd.read_csv(drive_path+\"nlp-getting-started/train.csv\")\n",
        "df_X_train_val = df_train_val.drop(columns=['target'])\n",
        "df_y_train_val = df_train_val[['id', 'target']]\n",
        "df_X_test = pd.read_csv(drive_path+\"nlp-getting-started/test.csv\")\n",
        "df_y_test = pd.read_csv(drive_path+\"nlp-getting-started/sample_submission.csv\")\n",
        "print(df_X_train_val.info(),df_y_train_val.info(),df_X_test.info(),df_y_test.info())\n",
        "# Get uniques classes\n",
        "labels=df_train_val['target'].unique()\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn5vVA7xbr94",
        "outputId": "c4f7ad8a-935a-49c0-90d2-fa6eda299392"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train.csv', 'sample_submission.csv', 'test.csv']\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 238.0+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype\n",
            "---  ------  --------------  -----\n",
            " 0   id      7613 non-null   int64\n",
            " 1   target  7613 non-null   int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 119.1 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3263 entries, 0 to 3262\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        3263 non-null   int64 \n",
            " 1   keyword   3237 non-null   object\n",
            " 2   location  2158 non-null   object\n",
            " 3   text      3263 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 102.1+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3263 entries, 0 to 3262\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype\n",
            "---  ------  --------------  -----\n",
            " 0   id      3263 non-null   int64\n",
            " 1   target  3263 non-null   int64\n",
            "dtypes: int64(2)\n",
            "memory usage: 51.1 KB\n",
            "None None None None\n",
            "[1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_train_val['num_words'] = df_X_train_val['text'].apply(lambda x: len(x.split()))\n",
        "# Identify outliers based on chosen criterion\n",
        "Q1 = df_X_train_val['num_words'].quantile(0.25)\n",
        "Q3 = df_X_train_val['num_words'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def clean_text(text, max_words=int(upper_bound)):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Get the list of English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Filter out stop words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Join the filtered words back into a sentence\n",
        "    text = ' '.join(filtered_words)\n",
        "    # Limit the number of words\n",
        "    text = ' '.join(text.split()[:max_words])\n",
        "    return text\n",
        "\n",
        "# Clean train and test data\n",
        "df_X_train_val['clean text']=df_X_train_val['text'].apply(clean_text)\n",
        "df_X_test['clean text']=df_X_test['text'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv3pg47ddulW",
        "outputId": "7a024be6-d1c9-4cae-f316-fc497865bec2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(df_X_train_val['clean text'], df_y_train_val['target'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ItcVtY_XjMDl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data to compute vocabulary size and maximum sequence length\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_X_train_val['clean text'])\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_train = pad_sequences(X_train, maxlen=int(upper_bound), padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=int(upper_bound), padding='post')\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "drAxL4xfigRj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(df_X_test['clean text'])\n",
        "X_test = pad_sequences(X_test, maxlen=int(upper_bound), padding='post')"
      ],
      "metadata": {
        "id": "JsO3BAcMt4M_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model building and training**"
      ],
      "metadata": {
        "id": "a8aiUWzdvaUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=int(upper_bound)))\n",
        "#model.add(SpatialDropout1D(0.5))\n",
        "model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "model.compile(Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics='accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-L8kLaegpsK",
        "outputId": "a3d57ace-28d4-42e2-cb20-b3852326a5e5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 31, 100)           2256400   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 128)               117248    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2382481 (9.09 MB)\n",
            "Trainable params: 2382225 (9.09 MB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs = 10, verbose=1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgEcQkK_gsLq",
        "outputId": "63cee370-9cb9-4345-84a2-6328ad071996"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "191/191 [==============================] - 25s 130ms/step - loss: 0.1291 - accuracy: 0.9537 - val_loss: 0.8069 - val_accuracy: 0.7525\n",
            "Epoch 2/10\n",
            "191/191 [==============================] - 25s 133ms/step - loss: 0.1139 - accuracy: 0.9586 - val_loss: 0.7351 - val_accuracy: 0.7597\n",
            "Epoch 3/10\n",
            "191/191 [==============================] - 25s 132ms/step - loss: 0.1037 - accuracy: 0.9647 - val_loss: 0.8626 - val_accuracy: 0.7761\n",
            "Epoch 4/10\n",
            "191/191 [==============================] - 23s 120ms/step - loss: 0.0984 - accuracy: 0.9678 - val_loss: 0.7833 - val_accuracy: 0.7479\n",
            "Epoch 5/10\n",
            "191/191 [==============================] - 24s 128ms/step - loss: 0.0850 - accuracy: 0.9716 - val_loss: 0.8538 - val_accuracy: 0.7722\n",
            "Epoch 6/10\n",
            "191/191 [==============================] - 25s 133ms/step - loss: 0.0727 - accuracy: 0.9765 - val_loss: 0.8555 - val_accuracy: 0.7669\n",
            "Epoch 7/10\n",
            "191/191 [==============================] - 25s 132ms/step - loss: 0.0705 - accuracy: 0.9754 - val_loss: 0.9629 - val_accuracy: 0.7564\n",
            "Epoch 8/10\n",
            "191/191 [==============================] - 26s 137ms/step - loss: 0.0726 - accuracy: 0.9742 - val_loss: 0.9478 - val_accuracy: 0.7708\n",
            "Epoch 9/10\n",
            "191/191 [==============================] - 30s 158ms/step - loss: 0.0664 - accuracy: 0.9777 - val_loss: 0.8415 - val_accuracy: 0.7597\n",
            "Epoch 10/10\n",
            "191/191 [==============================] - 31s 159ms/step - loss: 0.0529 - accuracy: 0.9816 - val_loss: 0.9463 - val_accuracy: 0.7636\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x786020bb1a80>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results**"
      ],
      "metadata": {
        "id": "OJ5mIuQcvdpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = model.predict(X_test)\n",
        "# Convert probabilities to binary class labels (0 or 1) based on a threshold\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "df_y_test['target'] = y_pred\n",
        "df_y_test.to_csv(drive_path+\"nlp-getting-started/my_sample_submission.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52WKz3K1lpro",
        "outputId": "4770fa1b-0054-4dec-db78-5ecf38ea9e60"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 1s 14ms/step\n"
          ]
        }
      ]
    }
  ]
}